import numpy as np

# Старые точки.
x_old = np.array([-9.93841085, -8.2398223, -9.06505398, -7.35203062, -5.82847285,
                  -5.08181713, -3.37174708, -3.6361873, -0.06175255, 0.09106786,
                  1.46721029, 0.41053496, 1.71012239, 1.84871104, 6.68526793,
                  6.82543486, 6.64741998, 8.01775519, 8.57773967, 11.8291112])
y_old = np.array([0.99747243, 3.28729745, 2.0644648, 2.88068415, -0.05454181,
                  0.63703982, 0.06238763, 0.25253028, 0.06582577, 0.05755049,
                  0.20686123, -0.03885818, 0.40837474, 0.52833438, 0.25072492,
                  0.26994154, 0.29157405, 0.52908138, -0.04000158, -0.98596774])

# Старое решение.
w_old = np.array([0.00965346, 0.0127624, -0.00155599, 0.03999249])

# Новые точки.
x_new = np.array([6.77483827, 8.25929536, -3.85651019, 9.23371979, -8.79233655,
                  0.18378788, -0.16105961, 1.36422926, -5.66964635, -9.17059911])
y_new = np.array([0.833035, 0.15730298, 1.16185828, 0.11734505, 3.82653602,
                  -0.23915503, 0.26089643, 0.04853463, 0.71751846, 2.70146699])


def f(X, y, w):
    return np.mean((X @ w - y) ** 2)


def grad(X, y, w):
    return 2 / len(X) * X.T @ (X @ w - y)


# Ваш код ниже.

# Склеиваем всё в единые векторы
x = np.concatenate([x_old, x_new])
y = np.concatenate([y_old, y_new])


# Составляем матрицу X.
X = np.stack([
    x ** 3,
    x ** 2,
    x,
    np.ones(len(x))
], axis=1)

# Инициализируем веса модели.
w = w_old

# Производим градиентный спуск.
gamma = 1e-6
max_iter = 10000
eps = 1e-6

# Делаем один шаг, чтобы проверить, что мы ещё не в минимуме.
f_old = f(X, y, w)
w = w - gamma * grad(X, y, w)
f_new = f(X, y, w)

# Текущий шаг.
i = 1

while np.abs(f_new - f_old) > eps and i < max_iter:
    # Обновляем веса.
    w = w - gamma * grad(X, y, w)

    # Увеличиваем номер шага и обновляем значения функции.
    i += 1
    f_old = f_new
    f_new = f(X, y, w)

result = w
print(result)
print(f(X, y, w))
