import numpy as np

# Исходные данные
x1 = np.array([0., 1., 2., 3., 4., 5., 6., 7., 8., 0., 1., 2., 3., 4., 5., 6., 7.,
               8., 0., 1., 2., 3., 4., 5., 6., 7., 8., 0., 1., 2., 3., 4., 5., 6.,
               7., 8., 0., 1., 2., 3., 4., 5., 6., 7., 8., 0., 1., 2., 3., 4.])
y1 = np.array([9., 7.5, 7.5, 10., 5.5, 7.5, 8.5, 5., 5.5, 5., 7.,
               8.5, 8.5, 5., 6.5, 8.5, 3.5, 3., 5.5, 6., 7., 7.,
               5.5, 6., 8.5, 7., 6., 6., 8., 10., 7., 6., 4.5,
               6.5, 4.5, 6.5, 7., 8.5, 6., 6.5, 9.5, 8., 8., 5.,
               3.5, 6., 9., 6.5, 5.5, 8.])

# Дополнительные данные
x2 = np.array([0.75, 1.5, 2.3, 4., 8., 0.6, 2.5, 4.3, 7.5, 0.5, 2., 7, 8])
y2 = np.array([9.75, 10.0, 8.5, 7.5, 6., 8.0, 8.0, 7.5, 6.8, 8.5, 9.5, 7.5, 6])

# Склеиваем всё в единые векторы
x = np.concatenate([x1, x2])
y = np.concatenate([y1, y2])


# Функция потерь и её градиент.
def f(X, y, w):
    return np.mean((X @ w - y) ** 2)


def grad(X, y, w):
    return 2 / len(X) * X.T @ (X @ w - y)


# Составляем обновлённую матрицу X.
X = np.stack([
    x ** 2,
    x,
    np.ones(len(x))
], axis=1)

# Берём старый результат в качестве начального приближения
# w = np.array([-0.11011743, 0.67058175, 6.44017367])
w = np.array([1, 2, 3])

# Производим градиентный спуск.
gamma = 1e-3
max_iter = 10000
eps = 1e-5

f_old = f(X, y, w)
w = w - grad(X, y, w) * gamma
f_new = f(X, y, w)
i = 0
while abs(f_old - f_new) > eps and i < max_iter:
    i = i + 1
    f_old = f_new
    w = w - grad(X, y, w) * gamma
    f_new = f(X, y, w)
result = w

print("Число выполненных итераций: ", i)
print(result)
print(f(X, y, w))
